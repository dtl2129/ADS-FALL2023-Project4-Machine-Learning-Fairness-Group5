{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEW_MobTuZmd",
        "outputId": "ed53a1e0-f59b-43fd-ee9c-965007720d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task A3: Maximizing fairness under accuracy constraints (gamma and Fine-gamma)**"
      ],
      "metadata": {
        "id": "yVa0GmGZneai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/2023fall/compas-scores-two-years.csv')"
      ],
      "metadata": {
        "id": "17wtnUv665Ln"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "0uAsVPCPukfq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training an unconstrained classifier on the biased data**\n",
        "\n",
        "We will train a logistic regression classifier on the data to see the correlations between the classifier decisions and sensitive feature value:\n",
        "\n",
        "Here we choose \"race\" as the sensitive attribute."
      ],
      "metadata": {
        "id": "8FYNIThFn1pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features for the model\n",
        "features = ['age', 'sex', 'priors_count']\n",
        "target = 'two_year_recid'\n",
        "sensitive_attr = 'race'\n",
        "\n",
        "# Preprocessing\n",
        "data_preprocessed = pd.get_dummies(data[features])\n",
        "x = data_preprocessed\n",
        "y = data[target]\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Retraining the unconstrained logistic regression classifier\n",
        "clf_unconstrained = LogisticRegression(solver='liblinear')\n",
        "clf_unconstrained.fit(x_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_unconstrained = clf_unconstrained.predict(x_test)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy_unconstrained = accuracy_score(y_test, y_pred_unconstrained)\n",
        "\n",
        "# Including the 'race' column in the test data for analysis\n",
        "data_test = pd.concat([x_test, data.loc[x_test.index, sensitive_attr], y_test], axis=1)\n",
        "\n",
        "# Calculating p-rule and covariance\n",
        "protected_group = data_test[sensitive_attr] == 'African-American'\n",
        "non_protected_group = data_test[sensitive_attr] != 'African-American'\n",
        "\n",
        "protected_positive_rate = np.mean(y_pred_unconstrained[protected_group])\n",
        "non_protected_positive_rate = np.mean(y_pred_unconstrained[non_protected_group])\n",
        "\n",
        "p_rule = min(protected_positive_rate / non_protected_positive_rate,\n",
        "             non_protected_positive_rate / protected_positive_rate) * 100\n",
        "\n",
        "race_binary = (data_test[sensitive_attr] == 'African-American').astype(int)\n",
        "covariance = np.cov(race_binary, y_pred_unconstrained)[0, 1]\n",
        "\n",
        "# Output results\n",
        "accuracy_unconstrained, p_rule, covariance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RofZkbMPvoYp",
        "outputId": "a1e4db19-d229-46dd-ca70-031f438953e8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6863741339491917, 52.595263532763525, 0.05455127575740759)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy of the classifier on the test set is approximately 68.64%.\n",
        "\n",
        "P-Rule: The p-rule achieved is about 52.60%. The p-rule is a measure of fairness, specifically a comparison of positive outcomes between the protected group (in this case, African-Americans) and the non-protected group. A p-rule of approximately 52.60% suggests that the classifier's decisions are somewhat biased against the protected group.\n",
        "\n",
        "These results imply that the classifier, when trained without fairness constraints, reflects the biases present in the data. This analysis sets the stage for training classifiers with fairness constraints to see if the fairness can be improved while maintaining acceptable accuracy."
      ],
      "metadata": {
        "id": "CNf-GZP77iUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_fairness_with_accuracy_constraints(model, x_test, y_test, sensitive_attr_binary, gamma=0.25):\n",
        "    \"\"\"\n",
        "    Optimize fairness subject to accuracy constraints.\n",
        "    Adjusts the decision threshold of the logistic regression model to balance fairness and accuracy.\n",
        "    \"\"\"\n",
        "    initial_accuracy = accuracy_score(y_test, model.predict(x_test))\n",
        "    target_accuracy = initial_accuracy * (1 - gamma)\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "\n",
        "    best_threshold = 0.5  # Initial decision threshold\n",
        "    best_p_rule = 0\n",
        "    best_covariance = float('inf')\n",
        "    best_accuracy = initial_accuracy\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Apply the threshold\n",
        "        y_pred_adjusted = (model.predict_proba(x_test)[:, 1] >= threshold).astype(int)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        current_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
        "        if current_accuracy < target_accuracy:\n",
        "            continue  # Skip if accuracy constraint is not met\n",
        "\n",
        "        # Calculate p-rule\n",
        "        protected_positive_rate = np.mean(y_pred_adjusted[sensitive_attr_binary == 1])\n",
        "        non_protected_positive_rate = np.mean(y_pred_adjusted[sensitive_attr_binary == 0])\n",
        "        if non_protected_positive_rate == 0:  # Avoid division by zero\n",
        "            continue\n",
        "\n",
        "        current_p_rule = min(protected_positive_rate / non_protected_positive_rate,\n",
        "                             non_protected_positive_rate / protected_positive_rate) * 100\n",
        "\n",
        "        # Calculate covariance\n",
        "        current_covariance = np.cov(sensitive_attr_binary, y_pred_adjusted)[0, 1]\n",
        "\n",
        "        # Update the best threshold if it has higher p-rule or lower covariance\n",
        "        if current_p_rule > best_p_rule or (current_p_rule == best_p_rule and abs(current_covariance) < abs(best_covariance)):\n",
        "            best_threshold = threshold\n",
        "            best_p_rule = current_p_rule\n",
        "            best_accuracy = current_accuracy\n",
        "            best_covariance = current_covariance\n",
        "\n",
        "    return best_threshold, best_accuracy, best_p_rule, best_covariance\n",
        "\n",
        "# Extracting the binary sensitive attribute (1 for African-American, 0 for others)\n",
        "sensitive_attr_binary = (data_test[sensitive_attr] == 'African-American').astype(int)\n",
        "\n",
        "# Optimizing fairness with accuracy constraint\n",
        "gamma_value = 0.25\n",
        "best_threshold, acc_fairness_optimized, p_rule_fairness_optimized, covariance_fairness_optimized = optimize_fairness_with_accuracy_constraints(\n",
        "    clf_unconstrained, x_test, y_test, sensitive_attr_binary, gamma=gamma_value\n",
        ")\n",
        "\n",
        "best_threshold, acc_fairness_optimized, p_rule_fairness_optimized, covariance_fairness_optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMSgo74j0I_I",
        "outputId": "6ea68c16-a6fe-4ece-d521-d1438a2c6c58"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24242424242424243,\n",
              " 0.5182448036951501,\n",
              " 87.73330863834369,\n",
              " 0.02883847805577727)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy with this threshold is about 51.82%, which is a large drop from the unconstrained model. This drop in accuracy is within the bounds of the 50% loss we are willing to accept (as dictated by gamma = 0.25).\n",
        "\n",
        "P-Rule: The p-rule achieved is 87.73%, indicating a high level of fairness according to this metric."
      ],
      "metadata": {
        "id": "akqoKW6bBABo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we choose \"sex\" as the sensitive attribute."
      ],
      "metadata": {
        "id": "qod7r1K25xdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sensitive_attr_sex = 'sex'\n",
        "\n",
        "# Preprocessing with 'sex' as the sensitive attribute\n",
        "data_preprocessed_sex = pd.get_dummies(data[features + [sensitive_attr_sex]])\n",
        "x_sex = data_preprocessed_sex.drop(columns=[sensitive_attr_sex + '_Female', sensitive_attr_sex + '_Male'])\n",
        "y_sex = data[target]\n",
        "\n",
        "# Splitting the data into training and testing sets for 'sex'\n",
        "x_train_sex, x_test_sex, y_train_sex, y_test_sex = train_test_split(x_sex, y_sex, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training the unconstrained logistic regression classifier with 'sex' as sensitive attribute\n",
        "clf_unconstrained_sex = LogisticRegression(solver='liblinear')\n",
        "clf_unconstrained_sex.fit(x_train_sex, y_train_sex)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_unconstrained_sex = clf_unconstrained_sex.predict(x_test_sex)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy_unconstrained_sex = accuracy_score(y_test_sex, y_pred_unconstrained_sex)\n",
        "\n",
        "# Including the 'sex' column in the test data for analysis\n",
        "data_test_sex = pd.concat([x_test_sex, data.loc[x_test_sex.index, sensitive_attr_sex], y_test_sex], axis=1)\n",
        "\n",
        "# Calculating p-rule and covariance for 'sex'\n",
        "protected_group_sex = data_test_sex[sensitive_attr_sex] == 'Female'\n",
        "non_protected_group_sex = data_test_sex[sensitive_attr_sex] != 'Female'\n",
        "\n",
        "protected_positive_rate_sex = np.mean(y_pred_unconstrained_sex[protected_group_sex])\n",
        "non_protected_positive_rate_sex = np.mean(y_pred_unconstrained_sex[non_protected_group_sex])\n",
        "\n",
        "p_rule_sex = min(protected_positive_rate_sex / non_protected_positive_rate_sex,\n",
        "                 non_protected_positive_rate_sex / protected_positive_rate_sex) * 100\n",
        "\n",
        "sex_binary = (data_test_sex[sensitive_attr_sex] == 'Female').astype(int)\n",
        "covariance_sex = np.cov(sex_binary, y_pred_unconstrained_sex)[0, 1]\n",
        "\n",
        "accuracy_unconstrained_sex, p_rule_sex, covariance_sex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wquv4g2iB1Ys",
        "outputId": "5590e1ba-cf7b-41d9-8cf2-08b438ea29ce"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6849884526558891, 61.09848252134914, -0.021624482930848257)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy of the classifier on the test set is approximately 68.50%.\n",
        "\n",
        "P-Rule: The p-rule achieved is about 61.10%. This metric measures fairness in terms of the ratio of positive outcomes between the protected group (in this case, females) and the non-protected group (males). A p-rule of approximately 61.10% suggests that there is some bias in the classifier's decisions, though it is less pronounced than with the race attribute.\n"
      ],
      "metadata": {
        "id": "I10AjO68CKLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features for the model - for simplicity, we use a few features\n",
        "features = ['age', 'priors_count']\n",
        "target = 'two_year_recid'\n",
        "sensitive_attr_sex = 'sex'\n",
        "\n",
        "# Preprocessing with 'sex' as the sensitive attribute\n",
        "data_preprocessed_sex = pd.get_dummies(data[features + [sensitive_attr_sex]])\n",
        "x_sex = data_preprocessed_sex.drop(columns=[sensitive_attr_sex + '_Female', sensitive_attr_sex + '_Male'])\n",
        "y_sex = data[target]\n",
        "\n",
        "# Splitting the data into training and testing sets for 'sex'\n",
        "x_train_sex, x_test_sex, y_train_sex, y_test_sex = train_test_split(x_sex, y_sex, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training the unconstrained logistic regression classifier with 'sex' as sensitive attribute\n",
        "clf_unconstrained_sex = LogisticRegression(solver='liblinear')\n",
        "clf_unconstrained_sex.fit(x_train_sex, y_train_sex)\n",
        "\n",
        "# Including the 'sex' column in the test data for analysis\n",
        "data_test_sex = pd.concat([x_test_sex, data.loc[x_test_sex.index, sensitive_attr_sex], y_test_sex], axis=1)\n",
        "\n",
        "# Extracting the binary sensitive attribute for 'sex' (1 for Female, 0 for Male)\n",
        "sensitive_attr_binary_sex = (data_test_sex[sensitive_attr_sex] == 'Female').astype(int)\n",
        "\n",
        "# Function to optimize fairness with accuracy constraints for 'sex'\n",
        "def optimize_fairness_sex_with_accuracy_constraints(model, x_test, y_test, sensitive_attr_binary, gamma=0.5):\n",
        "    initial_accuracy = accuracy_score(y_test, model.predict(x_test))\n",
        "    target_accuracy = initial_accuracy * (1 - gamma)\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "\n",
        "    best_threshold = 0.5  # Initial decision threshold\n",
        "    best_p_rule = 0\n",
        "    best_covariance = float('inf')\n",
        "    best_accuracy = initial_accuracy\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred_adjusted = (model.predict_proba(x_test)[:, 1] >= threshold).astype(int)\n",
        "        current_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
        "        if current_accuracy < target_accuracy:\n",
        "            continue\n",
        "\n",
        "        protected_positive_rate = np.mean(y_pred_adjusted[sensitive_attr_binary == 1])\n",
        "        non_protected_positive_rate = np.mean(y_pred_adjusted[sensitive_attr_binary == 0])\n",
        "        if non_protected_positive_rate == 0:\n",
        "            continue\n",
        "\n",
        "        current_p_rule = min(protected_positive_rate / non_protected_positive_rate,\n",
        "                             non_protected_positive_rate / protected_positive_rate) * 100\n",
        "\n",
        "        current_covariance = np.cov(sensitive_attr_binary, y_pred_adjusted)[0, 1]\n",
        "\n",
        "        if current_p_rule > best_p_rule or (current_p_rule == best_p_rule and abs(current_covariance) < abs(best_covariance)):\n",
        "            best_threshold = threshold\n",
        "            best_p_rule = current_p_rule\n",
        "            best_accuracy = current_accuracy\n",
        "            best_covariance = current_covariance\n",
        "\n",
        "    return best_threshold, best_accuracy, best_p_rule, best_covariance\n",
        "\n",
        "# Optimizing fairness with accuracy constraint for 'sex'\n",
        "gamma_value_sex = 0.2\n",
        "best_threshold_sex, acc_fairness_optimized_sex, p_rule_fairness_optimized_sex, covariance_fairness_optimized_sex = optimize_fairness_sex_with_accuracy_constraints(\n",
        "    clf_unconstrained_sex, x_test_sex, y_test_sex, sensitive_attr_binary_sex, gamma=gamma_value_sex\n",
        ")\n",
        "\n",
        "best_threshold_sex, acc_fairness_optimized_sex, p_rule_fairness_optimized_sex, covariance_fairness_optimized_sex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRmwLoZkCLm9",
        "outputId": "c7b662bb-b336-4a9a-ef7c-3c3d566f5beb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-43-1ed9eaa9e373>:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.30303030303030304,\n",
              " 0.5547344110854503,\n",
              " 94.85804669182075,\n",
              " -0.006300452929098003)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy of the classifier with this threshold is about 55.47%. This is higher than the accuracy we observed with gamma set to 0.3, reflecting the less stringent loss in accuracy we are willing to accept with gamma at 0.2.\n",
        "\n",
        "P-Rule: The p-rule achieved is approximately 94.86%, indicating a high level of fairness."
      ],
      "metadata": {
        "id": "KrHCMZYtDv4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping age categories\n",
        "age_cat_mapping = {'Less than 25': 0, '25 - 45': 1, 'Greater than 45': 2}\n",
        "data['age_cat_mapped'] = data['age_cat'].map(age_cat_mapping)\n",
        "\n",
        "# Selecting features for the model, excluding 'age' as it is represented by 'age_cat_mapped'\n",
        "features_age_sensitive = ['sex', 'priors_count', 'age_cat_mapped']\n",
        "sensitive_attr_age = 'age_cat_mapped'\n",
        "\n",
        "# Preprocessing with 'age_cat_mapped' as the sensitive attribute\n",
        "data_preprocessed_age = pd.get_dummies(data[features_age_sensitive])\n",
        "x_age = data_preprocessed_age.drop(columns=['age_cat_mapped'])\n",
        "y_age = data[target]\n",
        "\n",
        "# Splitting the data into training and testing sets for 'age_cat_mapped'\n",
        "x_train_age, x_test_age, y_train_age, y_test_age = train_test_split(x_age, y_age, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training the unconstrained logistic regression classifier with 'age_cat_mapped' as sensitive attribute\n",
        "clf_unconstrained_age = LogisticRegression(solver='liblinear')\n",
        "clf_unconstrained_age.fit(x_train_age, y_train_age)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_unconstrained_age = clf_unconstrained_age.predict(x_test_age)\n",
        "\n",
        "# Calculating accuracy\n",
        "accuracy_unconstrained_age = accuracy_score(y_test_age, y_pred_unconstrained_age)\n",
        "\n",
        "# Including the 'age_cat_mapped' column in the test data for analysis\n",
        "data_test_age = pd.concat([x_test_age, data.loc[x_test_age.index, sensitive_attr_age], y_test_age], axis=1)\n",
        "\n",
        "# Calculating p-rule and covariance for 'age_cat_mapped'\n",
        "# Here, we consider each age category as a protected group one at a time\n",
        "p_rules_age = {}\n",
        "covariances_age = {}\n",
        "for age_group in age_cat_mapping.values():\n",
        "    protected_group_age = data_test_age[sensitive_attr_age] == age_group\n",
        "    non_protected_group_age = data_test_age[sensitive_attr_age] != age_group\n",
        "\n",
        "    protected_positive_rate_age = np.mean(y_pred_unconstrained_age[protected_group_age])\n",
        "    non_protected_positive_rate_age = np.mean(y_pred_unconstrained_age[non_protected_group_age])\n",
        "\n",
        "    p_rules_age[age_group] = min(protected_positive_rate_age / non_protected_positive_rate_age,\n",
        "                                 non_protected_positive_rate_age / protected_positive_rate_age) * 100\n",
        "\n",
        "    age_binary = (data_test_age[sensitive_attr_age] == age_group).astype(int)\n",
        "    covariances_age[age_group] = np.cov(age_binary, y_pred_unconstrained_age)[0, 1]\n",
        "\n",
        "accuracy_unconstrained_age, p_rules_age, covariances_age"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M40DFdCgHVqS",
        "outputId": "8ef53d54-86e8-4a7a-f48a-9af0e987d606"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6475750577367205,\n",
              " {0: 32.850691864274125, 1: 65.0253908496099, 2: 83.14940057425123},\n",
              " {0: -0.03259659428054287, 1: 0.024667346842943305, 2: 0.00792924743759951})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy of the classifier on the test set is approximately 64.76%.\n",
        "\n",
        "P-Rule for Different Age Categories: For the group 'Less than 25': The p-rule is about 32.85%, indicating a significant bias against this age group.\n",
        "For the group '25 - 45': The p-rule is about 65.03%, suggesting some bias but less severe than the youngest group.\n",
        "For the group 'Greater than 45': The p-rule is about 83.15%, indicating relatively less bias compared to the other groups.\n",
        "Covariance between Age Categories and Decision Boundary:\n",
        "\n",
        "For the group 'Less than 25': The covariance is approximately -0.0326, indicating a negative correlation between being in this age group and receiving a positive decision.\n",
        "For the group '25 - 45': The covariance is approximately 0.0247, suggesting a slight positive correlation.\n",
        "For the group 'Greater than 45': The covariance is approximately 0.0079, indicating a very small positive correlation."
      ],
      "metadata": {
        "id": "ZFtFiJZQHaY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_fairness_age_with_accuracy_constraints(model, x_test, y_test, sensitive_attr_data, age_categories, gamma=0.15):\n",
        "    \"\"\"\n",
        "    Optimize fairness with respect to age categories subject to accuracy constraints.\n",
        "    \"\"\"\n",
        "    initial_accuracy = accuracy_score(y_test, model.predict(x_test))\n",
        "    target_accuracy = initial_accuracy * (1 - gamma)\n",
        "    thresholds = np.linspace(0, 1, 100)\n",
        "\n",
        "    best_results = {}\n",
        "\n",
        "    for age_group in age_categories:\n",
        "        best_threshold = 0.5  # Initial decision threshold\n",
        "        best_p_rule = 0\n",
        "        best_covariance = float('inf')\n",
        "        best_accuracy = initial_accuracy\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            # Apply the threshold\n",
        "            y_pred_adjusted = (model.predict_proba(x_test)[:, 1] >= threshold).astype(int)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            current_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
        "            if current_accuracy < target_accuracy:\n",
        "                continue  # Skip if accuracy constraint is not met\n",
        "\n",
        "            # Calculate p-rule and covariance for the current age group\n",
        "            protected_group = sensitive_attr_data == age_group\n",
        "            non_protected_group = sensitive_attr_data != age_group\n",
        "\n",
        "            protected_positive_rate = np.mean(y_pred_adjusted[protected_group])\n",
        "            non_protected_positive_rate = np.mean(y_pred_adjusted[non_protected_group])\n",
        "            if non_protected_positive_rate == 0:  # Avoid division by zero\n",
        "                continue\n",
        "\n",
        "            current_p_rule = min(protected_positive_rate / non_protected_positive_rate,\n",
        "                                 non_protected_positive_rate / protected_positive_rate) * 100\n",
        "            current_covariance = np.cov(protected_group, y_pred_adjusted)[0, 1]\n",
        "\n",
        "            # Update the best threshold if it has higher p-rule or lower covariance\n",
        "            if current_p_rule > best_p_rule or (current_p_rule == best_p_rule and abs(current_covariance) < abs(best_covariance)):\n",
        "                best_threshold = threshold\n",
        "                best_p_rule = current_p_rule\n",
        "                best_accuracy = current_accuracy\n",
        "                best_covariance = current_covariance\n",
        "\n",
        "        best_results[age_group] = {\n",
        "            'threshold': best_threshold,\n",
        "            'accuracy': best_accuracy,\n",
        "            'p_rule': best_p_rule,\n",
        "            'covariance': best_covariance\n",
        "        }\n",
        "\n",
        "    return best_results\n",
        "\n",
        "# Optimizing fairness with accuracy constraint for 'age_cat_mapped'\n",
        "gamma_value_age = 0.15\n",
        "optimized_results_age = optimize_fairness_age_with_accuracy_constraints(\n",
        "    clf_unconstrained_age, x_test_age, y_test_age, data_test_age[sensitive_attr_age], age_cat_mapping.values(), gamma=gamma_value_age\n",
        ")\n",
        "\n",
        "optimized_results_age\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs3MnSxsImB6",
        "outputId": "23668869-bd67-4a02-d9c8-8e4d22039381"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n",
            "<ipython-input-45-6993f22ffcdf>:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  non_protected_positive_rate / protected_positive_rate) * 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'threshold': 0.38383838383838387,\n",
              "  'accuracy': 0.5981524249422633,\n",
              "  'p_rule': 80.32585252450748,\n",
              "  'covariance': -0.021748494149488005},\n",
              " 1: {'threshold': 0.797979797979798,\n",
              "  'accuracy': 0.5796766743648961,\n",
              "  'p_rule': 98.67822318526544,\n",
              "  'covariance': 0.00011718099661475575},\n",
              " 2: {'threshold': 0.37373737373737376,\n",
              "  'accuracy': 0.5939953810623556,\n",
              "  'p_rule': 97.92574724448001,\n",
              "  'covariance': 0.00222451793573615}}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Age Group 'Less than 25':\n",
        "\n",
        "Best Decision Threshold: Approximately 0.384.\n",
        "Accuracy: About 59.82%.\n",
        "P-Rule: Approximately 80.33%, indicating improved fairness compared to the unconstrained model.\n",
        "\n",
        "\n",
        "Age Group '25 - 45':\n",
        "Best Decision Threshold: Approximately 0.798.\n",
        "Accuracy: About 57.97%.\n",
        "P-Rule: Approximately 98.68%, indicating very high fairness.\n",
        "\n",
        "Best Decision Threshold: Approximately 0.374.\n",
        "Accuracy: About 59.40%.\n",
        "P-Rule: Approximately 97.93%, also indicating very high fairness.\n",
        "Covariance: Approximately 0.0022, indicating a minimal positive correlation.\n",
        "\n",
        "These results demonstrate that with a gamma value of 0.15, the model achieves a better balance between fairness and accuracy across different age groups compared to the unconstrained model. The p-rule values are significantly higher, suggesting less bias, especially for the younger age group, which had the most significant bias in the unconstrained model."
      ],
      "metadata": {
        "id": "YjyHiUilIrJM"
      }
    }
  ]
}